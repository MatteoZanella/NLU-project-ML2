{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU-project-ML2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoZanella/NLU-project-ML2/blob/main/NLU_project_ML2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9jDvo0yPfE7"
      },
      "source": [
        "# NLU Project - ML2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQBp3krgPabY"
      },
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvbMpVrUpcrH"
      },
      "source": [
        "Download the Penn Treebank Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFk8G7F37Hbl"
      },
      "source": [
        "%%capture\n",
        "!wget -nc https://data.deepai.org/ptbdataset.zip\n",
        "!unzip -n ptbdataset.zip -d ptbdataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTpG2nGghRh"
      },
      "source": [
        "Download the pre-trained models (saved during the development of this project)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlQzHP11gp0H"
      },
      "source": [
        "%%capture\n",
        "!wget -nc https://github.com/MatteoZanella/NLU-project-ML2/raw/main/models/simple.zip\n",
        "!unzip -n simple.zip\n",
        "!wget -nc https://github.com/MatteoZanella/NLU-project-ML2/raw/main/models/dense.zip\n",
        "!unzip -n dense.zip\n",
        "!wget -nc https://github.com/MatteoZanella/NLU-project-ML2/raw/main/models/complex.zip\n",
        "!unzip -n complex.zip\n",
        "!wget -nc https://github.com/MatteoZanella/NLU-project-ML2/raw/main/models/huge_reverse.zip\n",
        "!unzip -n huge_reverse.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdTb05YWBbD"
      },
      "source": [
        "### Text processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ZjwOB3puF8"
      },
      "source": [
        "Load the data in a Dataset:\n",
        "- Tags substitution for uniformity\n",
        "  - Numbers: N -> [N]\n",
        "  - Unknown words: \\<unk\\> -> [UNK]\n",
        "  - Start and end of sentences: [S], [/S]\n",
        "- Create the targets as the sentences shifted by 1 (the target has a [/S] at the end instead of a [S] tag at the start)\n",
        "- Use a TextVectorization to convert the sentences into vectors of words, and to convert the words into integers\n",
        "- Shuffle the training set\n",
        "- Create padded minibatches: the shortest sentences are padded with zeros (the integer of the padding word) until they match the length of the longest sentence in the minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ6H5dvAiSg0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "458sPr4FQ73V"
      },
      "source": [
        "# Modify and add the tags, add the targets\n",
        "def add_tags_and_targets(sentence):\n",
        "  sentence = tf.strings.regex_replace(sentence, \" N \", \" [N] \")\n",
        "  sentence = tf.strings.regex_replace(sentence, \" <unk> \", \" [UNK] \")\n",
        "  return '[S]' + sentence, sentence + '[/S]'\n",
        "\n",
        "train = tf.data.TextLineDataset('ptbdataset/ptb.train.txt').map(add_tags_and_targets)\n",
        "valid = tf.data.TextLineDataset('ptbdataset/ptb.valid.txt').map(add_tags_and_targets)\n",
        "test = tf.data.TextLineDataset('ptbdataset/ptb.test.txt').map(add_tags_and_targets)\n",
        "\n",
        "# Training set: 42068 sentences\n",
        "# Validation set: 3370 sentences\n",
        "# Test set: 3761 sentences\n",
        "\n",
        "# Vectorize the dataset (use the input sentences with both [S], [/S])\n",
        "textVectorization = layers.TextVectorization(standardize=None)\n",
        "textVectorization.adapt(train.map(lambda x, y: x + '[/S]'))\n",
        "VOCABULARY_SIZE = textVectorization.vocabulary_size()\n",
        "\n",
        "# Shuffling\n",
        "BUFFER_SIZE = 42068 #Equal to training set size\n",
        "train = train.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train = train.map(lambda x, y: (textVectorization(x), textVectorization(y))).padded_batch(BATCH_SIZE)\n",
        "valid = valid.map(lambda x, y: (textVectorization(x), textVectorization(y))).padded_batch(128)\n",
        "test = test.map(lambda x, y: (textVectorization(x), textVectorization(y))).padded_batch(128)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMi4ldpqWadm"
      },
      "source": [
        "## Additional structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iczL9rAWgSv"
      },
      "source": [
        "### Loss function\n",
        "The Keras implementation for the sparse categorical cross-entropy function is not suited for time sequences having a shape (Batch size, Sequence length, Vocabulary). The problem is that it sets at zero the cross-entropy values by applying the masking, but then the reduction includes also the zero values in the mean\n",
        "\n",
        "This implementation:\n",
        "- Computes the cross-entropies without reduction\n",
        "- Masks out the values corresponding to padded words in the true labels\n",
        "- Computes the number of valid words for each sentence\n",
        "- Divides the cross-entropies by the number of valid words in the same sentence and multiplies by the length of the padded sentences\n",
        "\n",
        "This way, the mean value on the entire matrix (incuded zero padded values) corresponds to the mean of the within-sentence cross-entropy averages. Cross-entropies are already masked, even if it's not necessary, so that the perplexity can be easily computed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkITrlZjSCfY"
      },
      "source": [
        "def sequence_sparse_categorical_crossentropy(y_true, y_pred):\n",
        "  # Standard sparse categorical cross-entropy loss, not reduced\n",
        "  entropies = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  # Masking: do not consider padded values. Compute valid values per sample\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "  entropies = tf.multiply(entropies, mask)\n",
        "  valid_per_sample = tf.reshape(tf.reduce_sum(mask, axis=-1), (-1, 1))\n",
        "  sequence_length = tf.cast(tf.shape(mask)[1], tf.float32)\n",
        "  # Apply the numerical corrections: now a tf.reduce_mean(entropies) produces the correct result\n",
        "  return tf.multiply(tf.divide(entropies, valid_per_sample), sequence_length)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-53DGwfcZcK"
      },
      "source": [
        "### Perplexity\n",
        "The perplexity can be computed as the exponential of cross-entropy averaged across all sentences. A stateful class is necessary to get the real average cross-entropy and not the average of the averages of all minibatches (almost equal, but if minibatches have different sizes the result it's not the same).\n",
        "\n",
        "The cross-entopy in Keras is computed using the natural logarithm, so the formula to compute the perplexity uses the natural exponential."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VjRKJltRRtx"
      },
      "source": [
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='perplexity', **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.entropies_sum = self.add_weight(name='ce', initializer='zeros')\n",
        "    self.samples_count = self.add_weight(name='n', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # entropies.shape = (batch_size, sequences_length)\n",
        "    entropies = sequence_sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    batch_size = tf.cast(tf.shape(entropies)[0], dtype=tf.float32)\n",
        "    entropies_sum = tf.multiply(tf.reduce_mean(entropies), batch_size)\n",
        "    self.samples_count.assign_add(batch_size)\n",
        "    self.entropies_sum.assign_add(entropies_sum)\n",
        "\n",
        "  def result(self):\n",
        "    # Perplexity over all samples (sentences), as the exp of the average entropy\n",
        "    return tf.math.exp(tf.divide(self.entropies_sum, self.samples_count))\n",
        "  \n",
        "  def reset_state(self):\n",
        "      # The state of the metric will be reset at the start of each epoch.\n",
        "      self.entropies_sum.assign(0.)\n",
        "      self.samples_count.assign(0.)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKApmLheaOAL"
      },
      "source": [
        "## RNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvRRR8xrajAG"
      },
      "source": [
        "custom_objects = {\"Perplexity\": Perplexity, \"sequence_sparse_categorical_crossentropy\": sequence_sparse_categorical_crossentropy}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YItVlTgZaY5_"
      },
      "source": [
        "### Simple model (1)\n",
        "Using a ReLU activation function in the GRU units seems to help a convergence in a smaller number of epoches, but the execution becomes slower because the efficient cuDNN implementation can't be used. Overall, it's preferrable to keep the tanh activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t657FWNRS1RX",
        "outputId": "4be851cb-f070-4ff0-b4b3-820bfd04b7a8"
      },
      "source": [
        "model_1 = tf.keras.Sequential(name='simple')\n",
        "# Input: out input have None (unknown, but equal because padded within the batch) time steps/words length. Single features\n",
        "model_1.add(layers.InputLayer(input_shape=(None,)))\n",
        "# Masking: do not consider 0 values (padding)\n",
        "# Embedding/reshaping: the word index is converted into a feature vector\n",
        "model_1.add(layers.Embedding(input_dim=VOCABULARY_SIZE, output_dim=32, mask_zero=True))\n",
        "# Recurrent layers\n",
        "model_1.add(layers.GRU(64, dropout=.5, return_sequences=True))\n",
        "# Dense layers\n",
        "model_1.add(layers.Dense(VOCABULARY_SIZE, activation='softmax'))\n",
        "model_1.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"simple\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          320128    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 64)          18816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 10004)       650260    \n",
            "=================================================================\n",
            "Total params: 989,204\n",
            "Trainable params: 989,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o549RILgDdg"
      },
      "source": [
        "#### Pretrained valuation\n",
        "- Loss: 5.04849\n",
        "- Perplexity: 156"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvQB4IE3gHuU",
        "outputId": "40421670-9c80-4693-cd48-88384ad53db1"
      },
      "source": [
        "test_model = tf.keras.models.load_model('simple', custom_objects=custom_objects)\n",
        "test_model.evaluate(test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 41s 1s/step - loss: 5.0485 - perplexity: 155.7876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.0484938621521, 155.7876434326172]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSHp7P68cqNN"
      },
      "source": [
        "### Dense model (2)\n",
        "Since stacking RNN layers was worsening the perplexity of the model, I tried using skip-connections inspired by the DenseNet architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMyMBNghoSma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ac77aa-a03d-4808-b5bb-f896b22e0f1b"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(None,))\n",
        "x = layers.Embedding(VOCABULARY_SIZE, output_dim=512, mask_zero=True)(inputs)\n",
        "for i in range(2):\n",
        "  gru = layers.GRU(512, dropout=.4, return_sequences=True)(x)\n",
        "  x = layers.concatenate([x, gru])\n",
        "x = layers.Dropout(.4)(x)\n",
        "outputs = layers.Dense(VOCABULARY_SIZE, activation='softmax')(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs, name='dense')\n",
        "model_2.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"dense\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 512)    5122048     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, None, 512)    1575936     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, 1024)   0           embedding_1[0][0]                \n",
            "                                                                 gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     (None, None, 512)    2362368     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, 1536)   0           concatenate[0][0]                \n",
            "                                                                 gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 1536)   0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 10004)  15376148    dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,436,500\n",
            "Trainable params: 24,436,500\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQluyqS3nlT9"
      },
      "source": [
        "#### Pretrained valuation\n",
        "- Loss: 4.73937\n",
        "- Perplexity: 114"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLW8NVW8nlUE",
        "outputId": "0546a1a3-64e9-40b0-aea0-77aab2dedf37"
      },
      "source": [
        "test_model = tf.keras.models.load_model('dense', custom_objects=custom_objects)\n",
        "test_model.evaluate(test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 162s 5s/step - loss: 4.7394 - perplexity: 114.3627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.739375114440918, 114.36271667480469]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpO891Y4f3s6"
      },
      "source": [
        "### Complex model (3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpeTlU9cb74Y",
        "outputId": "c1b76c89-b8fe-44fc-eec4-e643c36b7c5d"
      },
      "source": [
        "LAYERS_COUNT = 2\n",
        "DROPOUT = .5\n",
        "RNN_UNITS = 650\n",
        "\n",
        "model_3 = tf.keras.Sequential(name='complex')\n",
        "model_3.add(layers.InputLayer(input_shape=(None,)))\n",
        "model_3.add(layers.Embedding(VOCABULARY_SIZE, RNN_UNITS, mask_zero=True))\n",
        "for i in range(LAYERS_COUNT):\n",
        "  model_3.add(layers.GRU(RNN_UNITS, dropout=DROPOUT, return_sequences=True))\n",
        "# Dense layers\n",
        "model_3.add(layers.Dropout(DROPOUT))\n",
        "model_3.add(layers.Dense(VOCABULARY_SIZE, activation='softmax'))\n",
        "model_3.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"complex\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 650)         6502600   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, None, 650)         2538900   \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, None, 650)         2538900   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 650)         0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, None, 10004)       6512604   \n",
            "=================================================================\n",
            "Total params: 18,093,004\n",
            "Trainable params: 18,093,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZyKG1PljjcL"
      },
      "source": [
        "#### Pretrained valuation\n",
        "The number of parameters of this network is a bit lower than the dense model, but the perplexity is about the same value (slighly worse)\n",
        "- Loss: 4.78133\n",
        "- Perplexity: 119"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL9f2eEdjjcM",
        "outputId": "c187de4e-d84f-4813-c948-73ca54d2b32f"
      },
      "source": [
        "test_model = tf.keras.models.load_model('complex', custom_objects=custom_objects)\n",
        "test_model.evaluate(test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 119s 4s/step - loss: 4.7813 - perplexity: 119.2632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.781332969665527, 119.26322174072266]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbCGSHzAfgCN"
      },
      "source": [
        "### Complex+ReverseEmbedding model (4)\n",
        "Like the complex model, but with an extra regularization on the final decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8x1RWqcgAgY"
      },
      "source": [
        "class ReverseEmbedding(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, embedding_layer):\n",
        "      super().__init__()\n",
        "      self.embedding_layer = embedding_layer\n",
        "\n",
        "    def __call__(self, x):\n",
        "        target = tf.transpose(self.embedding_layer.embeddings)\n",
        "        return tf.norm(target - x)\n",
        "    \n",
        "    def get_config(self):\n",
        "      return {\"embedding_layer\": self.embedding_layer}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy_3KhFnh_f0",
        "outputId": "9269e0b6-07f4-4b20-a2e1-5c2cb30b32c4"
      },
      "source": [
        "LAYERS_COUNT = 2\n",
        "DROPOUT = .5\n",
        "RNN_UNITS = 650\n",
        "\n",
        "model_4 = tf.keras.Sequential(name='reverse')\n",
        "model_4.add(layers.InputLayer(input_shape=(None,)))\n",
        "embedding_layer = layers.Embedding(VOCABULARY_SIZE, RNN_UNITS, mask_zero=True)\n",
        "model_4.add(embedding_layer)\n",
        "for i in range(LAYERS_COUNT):\n",
        "  model_4.add(layers.GRU(RNN_UNITS, dropout=DROPOUT, return_sequences=True))\n",
        "# Dense layers\n",
        "model_4.add(layers.Dropout(DROPOUT))\n",
        "model_4.add(layers.Dense(VOCABULARY_SIZE, kernel_regularizer=ReverseEmbedding(embedding_layer), activation='softmax'))\n",
        "model_4.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"reverse\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 650)         6502600   \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, None, 650)         2538900   \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, None, 650)         2538900   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 650)         0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 10004)       6512604   \n",
            "=================================================================\n",
            "Total params: 18,093,004\n",
            "Trainable params: 18,093,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_PkVlfKwuf"
      },
      "source": [
        "#### Pretrained valuation\n",
        "The regularization term increases the loss value. The perplexity is slightly better than the complex model. The training is smoother, less increments of validation perplexity on subsequent epochs\n",
        "- Loss: 7.23776\n",
        "- Perplexity: 116\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b9xg24tKwu6"
      },
      "source": [
        "# For some reasons, this model is too heavy to be saved on GitHub"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWU1nBfQ6fmg"
      },
      "source": [
        "### Huge+ReverseEmbedding model (5)\n",
        "\n",
        "The complex model  with an increased number of RNN units complemented by the ReverseEmbedding regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS9l6Rzl6fmh",
        "outputId": "10e2cf6a-f61f-43e5-accc-9c42f7662c73"
      },
      "source": [
        "LAYERS_COUNT = 2\n",
        "DROPOUT = .6\n",
        "RNN_UNITS = 880\n",
        "\n",
        "model_5 = tf.keras.Sequential(name='huge_reverse')\n",
        "model_5.add(layers.InputLayer(input_shape=(None,)))\n",
        "embedding_layer = layers.Embedding(VOCABULARY_SIZE, RNN_UNITS, mask_zero=True)\n",
        "model_5.add(embedding_layer)\n",
        "for i in range(LAYERS_COUNT):\n",
        "  model_5.add(layers.GRU(RNN_UNITS, dropout=DROPOUT, return_sequences=True))\n",
        "# Dense layers\n",
        "model_5.add(layers.Dropout(DROPOUT))\n",
        "model_5.add(layers.Dense(VOCABULARY_SIZE, kernel_regularizer=ReverseEmbedding(embedding_layer), activation='softmax'))\n",
        "model_5.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"huge_reverse\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 880)         8803520   \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, None, 880)         4651680   \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, None, 880)         4651680   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 880)         0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, None, 10004)       8813524   \n",
            "=================================================================\n",
            "Total params: 26,920,404\n",
            "Trainable params: 26,920,404\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izX-U3RxMSIY"
      },
      "source": [
        "#### Pretrained valuation\n",
        "The improvement in the perplexity is limited. The loss includes the regularization.\n",
        "- Loss: 7.11412\n",
        "- Perplexity: 111"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iszjJTKvMSIZ",
        "outputId": "c224b60f-661b-4352-edfd-a05584c9d1c4"
      },
      "source": [
        "test_model = tf.keras.models.load_model('huge_reverse', custom_objects=custom_objects)\n",
        "test_model.evaluate(test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 182s 6s/step - loss: 7.1141 - perplexity: 111.1599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.114117622375488, 111.15992736816406]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIHQvQ3Vv-Vu"
      },
      "source": [
        "## Language models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwW099kh0-gx"
      },
      "source": [
        "def language_model(model, vocabulary):\n",
        "  lm = tf.keras.Sequential(name=f\"{model.name}_LM\")\n",
        "  # New textVectorization with same vocabulary but with normalization\n",
        "  lm.add(layers.TextVectorization(vocabulary=vocabulary))\n",
        "  lm.add(model)\n",
        "  # VectorTextification(textVectorization.get_vocabulary())\n",
        "  lm.add(layers.Lambda(lambda x: tf.argmax(x[:, -1, :], axis=-1)))\n",
        "  lm.add(layers.StringLookup(invert=True, mask_token='', vocabulary= textVectorization.get_vocabulary()[2:]))\n",
        "  return lm"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Aqef_MOzNXs",
        "outputId": "7c04ee41-de2e-42f2-bbdc-44350d9bc6ff"
      },
      "source": [
        "lm = language_model(test_model, textVectorization.get_vocabulary())\n",
        "lm.predict([\"The investments of the company raised in the last\"])"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'year'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MSHZqGjpTvz"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMOtwVfkroA1"
      },
      "source": [
        "### Training\n",
        "The SGD optimizer performs significantly better than Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-3TW0hCzNaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72be7f3c-5d38-402d-a65e-4de1a82173cf"
      },
      "source": [
        "callbacks = [\n",
        "  tf.keras.callbacks.ReduceLROnPlateau('val_perplexity', factor=0.8, min_delta=1.),\n",
        "  tf.keras.callbacks.EarlyStopping(\"val_perplexity\", min_delta=1., patience=10, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "model = model_5\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1., clipnorm=10.), \n",
        "              loss=sequence_sparse_categorical_crossentropy, \n",
        "              metrics=[Perplexity()])\n",
        "model.fit(train, epochs=60, validation_data=valid, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "2630/2630 [==============================] - 133s 48ms/step - loss: 8.4759 - perplexity: 605.2038 - val_loss: 8.0129 - val_perplexity: 380.2131\n",
            "Epoch 2/60\n",
            "2630/2630 [==============================] - 126s 47ms/step - loss: 7.0133 - perplexity: 310.7570 - val_loss: 7.4282 - val_perplexity: 246.3510\n",
            "Epoch 3/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 6.7954 - perplexity: 241.8979 - val_loss: 7.4658 - val_perplexity: 209.2040\n",
            "Epoch 4/60\n",
            "2630/2630 [==============================] - 126s 47ms/step - loss: 6.6512 - perplexity: 205.2070 - val_loss: 7.5510 - val_perplexity: 197.9425\n",
            "Epoch 5/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 6.5410 - perplexity: 181.2681 - val_loss: 7.9574 - val_perplexity: 171.2652\n",
            "Epoch 6/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 6.4544 - perplexity: 164.1912 - val_loss: 7.8297 - val_perplexity: 158.8089\n",
            "Epoch 7/60\n",
            "2630/2630 [==============================] - 126s 47ms/step - loss: 6.3831 - perplexity: 151.2000 - val_loss: 7.5760 - val_perplexity: 146.8949\n",
            "Epoch 8/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 6.3186 - perplexity: 140.4484 - val_loss: 7.4891 - val_perplexity: 152.3990\n",
            "Epoch 9/60\n",
            "2630/2630 [==============================] - 126s 47ms/step - loss: 6.2665 - perplexity: 132.1524 - val_loss: 7.4651 - val_perplexity: 134.1884\n",
            "Epoch 10/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 6.2195 - perplexity: 125.3136 - val_loss: 7.0136 - val_perplexity: 127.1839\n",
            "Epoch 11/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 6.1748 - perplexity: 119.0473 - val_loss: 7.4517 - val_perplexity: 128.2366\n",
            "Epoch 12/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 6.1365 - perplexity: 114.0246 - val_loss: 7.3450 - val_perplexity: 126.7312\n",
            "Epoch 13/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 6.0986 - perplexity: 109.2852 - val_loss: 7.8183 - val_perplexity: 122.0224\n",
            "Epoch 14/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 6.0669 - perplexity: 105.4114 - val_loss: 8.7709 - val_perplexity: 134.1987\n",
            "Epoch 15/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 6.0360 - perplexity: 101.8463 - val_loss: 8.1219 - val_perplexity: 129.6723\n",
            "Epoch 16/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 6.0055 - perplexity: 98.4976 - val_loss: 7.0258 - val_perplexity: 117.6926\n",
            "Epoch 17/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.9769 - perplexity: 95.4449 - val_loss: 7.4020 - val_perplexity: 124.1546\n",
            "Epoch 18/60\n",
            "2630/2630 [==============================] - 127s 48ms/step - loss: 5.9508 - perplexity: 92.9811 - val_loss: 7.2630 - val_perplexity: 114.5063\n",
            "Epoch 19/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.9256 - perplexity: 90.3247 - val_loss: 7.4443 - val_perplexity: 118.8592\n",
            "Epoch 20/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.9027 - perplexity: 88.2045 - val_loss: 7.0413 - val_perplexity: 114.1496\n",
            "Epoch 21/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.8776 - perplexity: 85.9911 - val_loss: 7.1232 - val_perplexity: 112.1715\n",
            "Epoch 22/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.8564 - perplexity: 84.0543 - val_loss: 7.5879 - val_perplexity: 114.5857\n",
            "Epoch 23/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.8349 - perplexity: 82.2672 - val_loss: 7.4356 - val_perplexity: 114.0544\n",
            "Epoch 24/60\n",
            "2630/2630 [==============================] - 126s 48ms/step - loss: 5.8127 - perplexity: 80.5193 - val_loss: 6.8879 - val_perplexity: 114.1037\n",
            "Epoch 25/60\n",
            "2630/2630 [==============================] - 126s 47ms/step - loss: 5.7923 - perplexity: 78.8536 - val_loss: 7.3696 - val_perplexity: 112.3602\n",
            "Epoch 26/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 5.7764 - perplexity: 77.5395 - val_loss: 7.4712 - val_perplexity: 119.5581\n",
            "Epoch 27/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 5.7585 - perplexity: 76.2947 - val_loss: 7.2101 - val_perplexity: 113.4787\n",
            "Epoch 28/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 5.7382 - perplexity: 74.8189 - val_loss: 7.1380 - val_perplexity: 111.6277\n",
            "Epoch 29/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 5.7200 - perplexity: 73.5147 - val_loss: 7.5227 - val_perplexity: 114.3294\n",
            "Epoch 30/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 5.7025 - perplexity: 72.2123 - val_loss: 7.7933 - val_perplexity: 117.5621\n",
            "Epoch 31/60\n",
            "2630/2630 [==============================] - 125s 47ms/step - loss: 5.6880 - perplexity: 71.2687 - val_loss: 7.8902 - val_perplexity: 115.7175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f41fbed8f50>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyZS2ByZrq1x"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6NaISjA5LzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e283c04-7eb1-40c0-c20f-906ae493a4d6"
      },
      "source": [
        "model.evaluate(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "236/236 [==============================] - 5s 20ms/step - loss: 7.1141 - perplexity: 111.1594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.114120006561279, 111.1594467163086]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ELgnyvheYDb"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWM1pzn_Qv_x",
        "outputId": "40632c3c-0b93-4f89-d5ec-eb22ae7e505e"
      },
      "source": [
        "name = model.name\n",
        "model.save(name)\n",
        "!zip -r saved-model.zip $name/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_24_layer_call_fn, gru_cell_24_layer_call_and_return_conditional_losses, gru_cell_25_layer_call_fn, gru_cell_25_layer_call_and_return_conditional_losses, gru_cell_24_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: huge_reverse/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: huge_reverse/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  adding: huge_reverse/ (stored 0%)\n",
            "  adding: huge_reverse/keras_metadata.pb (deflated 91%)\n",
            "  adding: huge_reverse/assets/ (stored 0%)\n",
            "  adding: huge_reverse/saved_model.pb (deflated 90%)\n",
            "  adding: huge_reverse/variables/ (stored 0%)\n",
            "  adding: huge_reverse/variables/variables.index (deflated 54%)\n",
            "  adding: huge_reverse/variables/variables.data-00000-of-00001 (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}